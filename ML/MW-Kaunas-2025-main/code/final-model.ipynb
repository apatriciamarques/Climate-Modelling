{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 12358416,
     "sourceType": "datasetVersion",
     "datasetId": 7791486
    },
    {
     "sourceId": 12359539,
     "sourceType": "datasetVersion",
     "datasetId": 7792297
    }
   ],
   "dockerImageVersionId": 31041,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, max_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Bidirectional, Input\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from haversine import haversine, Unit\n",
    "import ast # To parse the coordinates string\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "CSV_FILE_PATH = '../data/raw/lithuanian_weather_custom_stations.csv'\n",
    "\n",
    "# Specify station codes. One will be the target, others will be inputs.\n",
    "TARGET_STATION = 'kauno-ams'\n",
    "INPUT_STATIONS = ['vilniaus-ams', 'klaipedos-ams']\n",
    "ALL_STATIONS = [TARGET_STATION] + INPUT_STATIONS\n",
    "\n",
    "# Features to be used for modeling\n",
    "# CLEANUP: 'windDirection' removed as it is now handled by cyclical features\n",
    "NUMERICAL_FEATURES = [\n",
    "    'airTemperature', 'windSpeed', 'windGust', 'seaLevelPressure', \n",
    "    'relativeHumidity', 'precipitation', 'cloudCover'\n",
    "]\n",
    "CATEGORICAL_FEATURES = ['conditionCode']\n",
    "TARGET_VARIABLE = 'airTemperature'\n",
    "\n",
    "\n",
    "# --- 2. DATA LOADING AND FILTERING ---\n",
    "print(\"Step 1: Loading and filtering data...\")\n",
    "\n",
    "try:\n",
    "    df_raw = pd.read_csv(CSV_FILE_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File '{CSV_FILE_PATH}' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Convert time and set as index\n",
    "df_raw['observationTimeUtc'] = pd.to_datetime(df_raw['observationTimeUtc'])\n",
    "df_raw.set_index('observationTimeUtc', inplace=True)\n",
    "\n",
    "# IMPORTANT: Filter data to keep only December for each year\n",
    "df_december = df_raw[df_raw.index.month == 12].copy()\n",
    "print(f\"Kept {len(df_december)} records for December only.\")\n",
    "\n",
    "# Filter data for the required stations only\n",
    "df = df_december[df_december['station_code'].isin(ALL_STATIONS)].copy()\n",
    "\n",
    "# Handle missing values\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Handle coordinates\n",
    "def parse_coords(coord_str):\n",
    "    try:\n",
    "        return ast.literal_eval(coord_str)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return {'latitude': np.nan, 'longitude': np.nan}\n",
    "df['coordinates'] = df['coordinates'].apply(parse_coords)\n",
    "coords = {station: df[df['station_code'] == station]['coordinates'].iloc[0] for station in ALL_STATIONS}\n",
    "\n",
    "# --- 3. FEATURE ENGINEERING ---\n",
    "print(\"\\nStep 2: Feature Engineering...\")\n",
    "# Handle wind direction\n",
    "wind_dir_rad = df['windDirection'] * np.pi / 180\n",
    "df['wind_sin'] = np.sin(wind_dir_rad)\n",
    "df['wind_cos'] = np.cos(wind_dir_rad)\n",
    "ALL_FEATURES = NUMERICAL_FEATURES + ['wind_sin', 'wind_cos']\n",
    "\n",
    "# One-Hot Encoding\n",
    "df = pd.get_dummies(df, columns=CATEGORICAL_FEATURES, prefix='condition')\n",
    "encoded_condition_cols = [col for col in df.columns if 'condition_' in col]\n",
    "ALL_FEATURES = ALL_FEATURES + encoded_condition_cols\n",
    "\n",
    "# --- 4. CREATING A PIVOT TABLE ---\n",
    "print(\"\\nStep 3: Creating a pivot table...\")\n",
    "pivot_df = df.pivot_table(index=df.index, columns='station_code', values=ALL_FEATURES)\n",
    "pivot_df.interpolate(method='time', inplace=True)\n",
    "pivot_df.fillna(method='ffill', inplace=True)\n",
    "pivot_df.fillna(method='bfill', inplace=True)\n",
    "pivot_df.columns = [f\"{col[1]}_{col[0]}\" for col in pivot_df.columns]\n",
    "pivot_df.sort_index(inplace=True)\n",
    "\n",
    "# Adding temporal features and distances\n",
    "df_index = pivot_df.index\n",
    "pivot_df['hour_sin'] = np.sin(2 * np.pi * df_index.hour / 24)\n",
    "pivot_df['hour_cos'] = np.cos(2 * np.pi * df_index.hour / 24)\n",
    "for station in INPUT_STATIONS:\n",
    "    distance = haversine((coords[TARGET_STATION]['latitude'], coords[TARGET_STATION]['longitude']), (coords[station]['latitude'], coords[station]['longitude']), unit=Unit.KILOMETERS)\n",
    "    pivot_df[f'dist_{TARGET_STATION}_to_{station}'] = distance\n",
    "\n",
    "# Autocorrelation (ACF/PACF) plots by year\n",
    "years = pivot_df.index.year.unique()\n",
    "for year in years:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "    fig.suptitle(f'Autocorrelation Analysis for {year}', fontsize=16)\n",
    "    year_data = pivot_df[pivot_df.index.year == year][target_temp_col].dropna()\n",
    "    plot_acf(year_data, ax=axes[0], lags=48)\n",
    "    plot_pacf(year_data, ax=axes[1], lags=48)\n",
    "    plt.savefig(f'acf_pacf_{year}.png')\n",
    "    plt.show()\n",
    "\n",
    "# --- 6. HYBRID MODEL: SARIMAX ---\n",
    "print(\"\\nStep 5: Modeling linear patterns with SARIMAX...\")\n",
    "exog_features_for_sarimax = ['airTemperature', 'windSpeed', 'seaLevelPressure', 'wind_sin', 'wind_cos']\n",
    "exog_cols = [col for col in pivot_df.columns if any(st in col for st in INPUT_STATIONS) and any(feat in col for feat in exog_features_for_sarimax)]\n",
    "exog_data = pivot_df[exog_cols]\n",
    "print(f\"Using {len(exog_cols)} exogenous features for SARIMAX.\")\n",
    "sarimax_model = SARIMAX(pivot_df[target_temp_col], exog=exog_data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 24))\n",
    "sarimax_results = sarimax_model.fit(disp=False)\n",
    "pivot_df['sarimax_pred'] = sarimax_results.predict(start=pivot_df.index[0], end=pivot_df.index[-1], exog=exog_data)\n",
    "pivot_df['residuals'] = pivot_df[target_temp_col] - pivot_df['sarimax_pred']\n",
    "print(\"SARIMAX model trained.\")\n",
    "\n",
    "# ADDED: Visualization of SARIMAX performance by year\n",
    "for year in years:\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.title(f'SARIMAX Predictions and Residuals for {year}')\n",
    "    \n",
    "    year_data = pivot_df[pivot_df.index.year == year]\n",
    "    \n",
    "    year_data[target_temp_col].plot(label='Actual Temperature', alpha=0.7)\n",
    "    year_data['sarimax_pred'].plot(label='SARIMAX Forecast', linestyle='--')\n",
    "    year_data['residuals'].plot(label='Residuals (Error)', color='green', alpha=0.5)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'sarimax_performance_{year}.png')\n",
    "    plt.show()\n",
    "\n",
    "# --- 7. DATA PREPARATION AND LSTM TRAINING ---\n",
    "print(\"\\nStep 6: Preparing data and training LSTM on residuals...\")\n",
    "lstm_input_cols = [col for col in pivot_df.columns if col not in ['sarimax_pred', 'residuals']]\n",
    "lstm_target_col = 'residuals'\n",
    "input_feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_input_features = input_feature_scaler.fit_transform(pivot_df[lstm_input_cols])\n",
    "residual_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_residuals = residual_scaler.fit_transform(pivot_df[[lstm_target_col]])\n",
    "\n",
    "def create_hybrid_dataset(input_data, target_data, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(input_data) - look_back):\n",
    "        dataX.append(input_data[i:(i + look_back), :])\n",
    "        dataY.append(target_data[i + look_back])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "look_back = 72 # 3 days\n",
    "X, y = create_hybrid_dataset(scaled_input_features, scaled_residuals, look_back)\n",
    "train_size = int(len(X) * 0.9)\n",
    "trainX, testX = X[0:train_size], X[train_size:len(X)]\n",
    "trainY, testY = y[0:train_size], y[train_size:len(y)]\n",
    "\n",
    "input_layer = Input(shape=(X.shape[1], X.shape[2]))\n",
    "conv1d_layer = Conv1D(filters=64, kernel_size=5, activation='relu')(input_layer)\n",
    "pool_layer = MaxPooling1D(pool_size=2)(conv1d_layer)\n",
    "bilstm_layer = Bidirectional(LSTM(128, return_sequences=True))(pool_layer)\n",
    "bilstm_layer = Dropout(0.3)(bilstm_layer)\n",
    "bilstm_layer_2 = Bidirectional(LSTM(64, return_sequences=False))(bilstm_layer)\n",
    "bilstm_layer_2 = Dropout(0.3)(bilstm_layer_2)\n",
    "dense_layer = Dense(50, activation='relu')(bilstm_layer_2)\n",
    "output_layer = Dense(1)(dense_layer)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "history = model.fit(trainX, trainY, epochs=50, batch_size=64, validation_data=(testX, testY), callbacks=[stop_early])\n",
    "print(\"LSTM model trained.\")\n",
    "\n",
    "# --- 8. MODEL BACKTESTING ---\n",
    "print(f\"\\nStep 7: Testing the model on data for 2023-12-15...\")\n",
    "BACKTEST_DATE = '2023-12-15'\n",
    "try:\n",
    "    backtest_start_dt = pd.to_datetime(BACKTEST_DATE)\n",
    "    if not any(pivot_df.index.date == backtest_start_dt.date()):\n",
    "         raise ValueError(f\"Date {BACKTEST_DATE} is missing from the dataset.\")\n",
    "\n",
    "    final_predictions = []\n",
    "    predicted_residuals = []\n",
    "    scaled_backtest_inputs = input_feature_scaler.transform(pivot_df[lstm_input_cols])\n",
    "    for hour in range(24):\n",
    "        current_dt = backtest_start_dt + pd.Timedelta(hours=hour)\n",
    "        input_end_idx = pivot_df.index.get_indexer([current_dt], method='ffill')[0]\n",
    "        input_start_idx = input_end_idx - look_back\n",
    "        input_sequence = scaled_backtest_inputs[input_start_idx:input_end_idx].reshape(1, look_back, len(lstm_input_cols))\n",
    "        predicted_scaled_residual = model.predict(input_sequence, verbose=0)\n",
    "        predicted_residual = residual_scaler.inverse_transform(predicted_scaled_residual)[0][0]\n",
    "        sarimax_prediction = pivot_df.loc[current_dt, 'sarimax_pred']\n",
    "        final_prediction = sarimax_prediction + predicted_residual\n",
    "        final_predictions.append(final_prediction)\n",
    "        predicted_residuals.append(predicted_residual)\n",
    "\n",
    "    results_df = pd.DataFrame(index=pd.date_range(start=backtest_start_dt, periods=24, freq='h'))\n",
    "    results_df['Actual_Temperature'] = pivot_df.loc[BACKTEST_DATE][target_temp_col].values\n",
    "    results_df['SARIMAX_Forecast'] = pivot_df.loc[BACKTEST_DATE]['sarimax_pred'].values\n",
    "    results_df['LSTM_Residual_Forecast'] = predicted_residuals\n",
    "    results_df['Hybrid_Forecast'] = final_predictions\n",
    "    results_df.dropna(inplace=True)\n",
    "\n",
    "    y_true = results_df['Actual_Temperature']\n",
    "    y_pred = results_df['Hybrid_Forecast']\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100 if np.all(y_true != 0) else np.inf\n",
    "    max_err = max_error(y_true, y_pred)\n",
    "    \n",
    "    metrics_data = {\n",
    "        'Metric': ['RMSE', 'MAE', 'R-squared', 'MAPE (%)', 'Max Error'],\n",
    "        'Value': [rmse, mae, r2, mape, max_err]\n",
    "    }\n",
    "    metrics_df = pd.DataFrame(metrics_data).round(4)\n",
    "    \n",
    "    print(\"\\n--- Quantitative Results ---\")\n",
    "    print(metrics_df)\n",
    "    \n",
    "    print(\"\\n--- Comparison Table ---\")\n",
    "    print(results_df.round(2))\n",
    "    \n",
    "    print(\"\\n--- LaTeX Output for Metrics ---\")\n",
    "    print(metrics_df.to_latex(index=False))\n",
    "    \n",
    "    print(\"\\n--- LaTeX Output for Comparison Table ---\")\n",
    "    print(results_df.round(2).to_latex(index=True))\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.title(f'Test: Forecast vs Actual Data for {TARGET_STATION} on {BACKTEST_DATE}')\n",
    "    plt.plot(results_df.index, results_df['Actual_Temperature'], label='Actual Temperature', color='blue', marker='.')\n",
    "    plt.plot(results_df.index, results_df['Hybrid_Forecast'], label='Hybrid Forecast', color='red', linestyle='--')\n",
    "    plt.plot(results_df.index, results_df['SARIMAX_Forecast'], label='SARIMAX Forecast', color='green', linestyle=':', alpha=0.7)\n",
    "    plt.xlabel('Date and Time')\n",
    "    plt.ylabel('Temperature, °C')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('backtest_forecast.png')\n",
    "    plt.show()\n",
    "\n",
    "    heatmap_data = results_df[['Actual_Temperature', 'Hybrid_Forecast']].copy()\n",
    "    heatmap_data.index = heatmap_data.index.strftime('%H:%M')\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.heatmap(heatmap_data.T, annot=True, fmt=\".1f\", cmap=\"coolwarm\", linewidths=.5)\n",
    "    plt.title(f'Heatmap of Actual vs. Forecast Temperatures on {BACKTEST_DATE}')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('')\n",
    "    plt.savefig('backtest_heatmap.png')\n",
    "    plt.show()\n",
    "\n",
    "except (KeyError, ValueError, IndexError) as e:\n",
    "    print(f\"ERROR during backtesting: {e}\")\n",
    "\n",
    "# --- 9. FINAL FORECAST FOR 2025-12-31 ---\n",
    "print(\"\\nStep 8: Final forecast for 2025-12-31...\")\n",
    "future_predictions = []\n",
    "last_known_input_sequence = scaled_input_features[-look_back:].reshape(1, look_back, len(lstm_input_cols))\n",
    "last_known_exog = exog_data.iloc[-1:].values.reshape(1, -1)\n",
    "\n",
    "sarimax_future_forecast = sarimax_results.get_forecast(steps=24, exog=np.repeat(last_known_exog, 24, axis=0))\n",
    "sarimax_future_preds = sarimax_future_forecast.predicted_mean\n",
    "\n",
    "current_sequence = last_known_input_sequence\n",
    "for i in range(24):\n",
    "    predicted_scaled_residual = model.predict(current_sequence, verbose=0)\n",
    "    predicted_residual = residual_scaler.inverse_transform(predicted_scaled_residual)[0][0]\n",
    "    \n",
    "    final_prediction = sarimax_future_preds.iloc[i] + predicted_residual\n",
    "    future_predictions.append(final_prediction)\n",
    "    \n",
    "    new_step = current_sequence[0, -1, :].copy() \n",
    "    current_sequence = np.append(current_sequence[:, 1:, :], [[new_step]], axis=1)\n",
    "\n",
    "\n",
    "forecast_df = pd.DataFrame(future_predictions, columns=['Forecast_Temperature'])\n",
    "forecast_df.index = pd.date_range(start='2025-12-31 00:00:00', periods=24, freq='h')\n",
    "\n",
    "print(\"Hourly forecast for 2025-12-31:\")\n",
    "print(forecast_df.round(2))\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.title(f'Final Forecast for {TARGET_STATION} on 2025-12-31')\n",
    "plt.plot(forecast_df.index, forecast_df['Forecast_Temperature'], label='Forecasted Temperature', color='red', marker='o')\n",
    "plt.xlabel('Date and Time')\n",
    "plt.ylabel('Temperature, °C')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_forecast_2025.png')\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-03T08:45:30.596253Z",
     "iopub.execute_input": "2025-07-03T08:45:30.597022Z",
     "iopub.status.idle": "2025-07-03T08:50:18.362258Z",
     "shell.execute_reply.started": "2025-07-03T08:45:30.596997Z",
     "shell.execute_reply": "2025-07-03T08:50:18.361503Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-07-03T09:10:08.708672Z",
     "start_time": "2025-07-03T09:09:08.357855Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001B[0m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 73\u001B[0m   \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_pywrap_tensorflow_internal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001B[39;00m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001B[39;00m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001B[39;00m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001B[39;00m\n\u001B[0;32m     78\u001B[0m \n\u001B[0;32m     79\u001B[0m \u001B[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001B[39;00m\n",
      "\u001B[1;31mImportError\u001B[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [2], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MinMaxScaler\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mean_squared_error, mean_absolute_error, r2_score, max_error\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Bidirectional, Input\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptimizers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Adam \n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py:40\u001B[0m\n\u001B[0;32m     37\u001B[0m _os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     39\u001B[0m \u001B[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001B[39;00m\n\u001B[1;32m---> 40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pywrap_tensorflow \u001B[38;5;28;01mas\u001B[39;00m _pywrap_tensorflow  \u001B[38;5;66;03m# pylint: disable=unused-import\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m module_util \u001B[38;5;28;01mas\u001B[39;00m _module_util\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlazy_loader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KerasLazyLoader \u001B[38;5;28;01mas\u001B[39;00m _KerasLazyLoader\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001B[0m\n\u001B[0;32m     86\u001B[0m     sys\u001B[38;5;241m.\u001B[39msetdlopenflags(_default_dlopen_flags)\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[1;32m---> 88\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m     89\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtraceback\u001B[38;5;241m.\u001B[39mformat_exc()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     90\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mFailed to load the native TensorFlow runtime.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     91\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSee https://www.tensorflow.org/install/errors \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     92\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfor some common causes and solutions.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     93\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIf you need help, create an issue \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     94\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     95\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mand include the entire stack trace above this error message.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mImportError\u001B[0m: Traceback (most recent call last):\n  File \"C:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "execution_count": 2
  }
 ]
}
